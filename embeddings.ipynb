{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds \n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n 'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n 'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\",  as_supervised = True, with_info = True) \n",
    "# imdb is a dict containing three different <tf.data.Dataset>,\n",
    "# each of which contain two tensors in the default format of (tensor containing the input, tensor containing the label)\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Large Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\nFeaturesDict({\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n    'text': Text(shape=(), dtype=tf.string),\n})\n{'test': <tfds.core.SplitInfo num_examples=25000>, 'train': <tfds.core.SplitInfo num_examples=25000>, 'unsupervised': <tfds.core.SplitInfo num_examples=50000>}\n"
    }
   ],
   "source": [
    "print(info.description)\n",
    "print(info.features)\n",
    "print(info.splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each of the 25000 records in the <tf.data.Dataset> (for the train set and test set) is stored as a FeaturesDict. \n",
    "* The FeaturesDict consists of a **string tensor** called \"text\" (containing the review) and an **integer tensor** called \"label\"(containing the label). \n",
    "    * Refer to https://www.tensorflow.org/guide/tensor info about string tensors in the FeaturesDict\n",
    "* We need to convert the string tensor and the integer tensor of each record to a np array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "reviews_train = []\n",
    "labels_train = []\n",
    "reviews_test = []\n",
    "labels_test = []\n",
    "\n",
    "for review, label in train_data:\n",
    "\n",
    "    reviews_train.append(review.numpy().decode(\"utf8\"))\n",
    "    # Tensors are explicitly converted to np arrays using their .numpy() method.\n",
    "    # review.numpy() is b\"This was an absolutely terrible movie. Don't ...\" , of <class 'bytes'>\n",
    "\n",
    "    # A string is a sequence of characters, i.e. unicode symbols that can't be directly stored on disk. \n",
    "    # A byte string is a sequence of bytes - things that can be stored on disk. \n",
    "    # The mapping between bytes and unicode symbols is an encoding.\n",
    "    # There are many different types of encodings. \n",
    "    # We need to convert the byte string to an actual string, using the decode() functoin of the byte string.\n",
    "    labels_train.append(label.numpy())\n",
    "\n",
    "for review, label in test_data:\n",
    "    reviews_test.append(review.numpy().decode(\"utf8\"))\n",
    "    labels_test.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n"
    }
   ],
   "source": [
    "print(reviews_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.int64'>\n"
    }
   ],
   "source": [
    "print(type(labels_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must convert the list of integers (labels) to np arrays,\n",
    "# because the input the the model must be np arrays.\n",
    "\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "# We do not have to explicitly convert the list of strings (reviews) to np arrays, \n",
    "# because the Tokenizer (which we will do below) takes in a list of strings and outputs a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, ..., 0, 1, 1])"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and vectorize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"UNK\" # \"UNK\" if the word is not part of the vocab_size words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token= oov_tok)\n",
    "tokenizer.fit_on_texts(reviews_train)\n",
    "word_index = tokenizer.word_index\n",
    "seqs_train = tokenizer.texts_to_sequences(reviews_train)\n",
    "padded_train = pad_sequences(seqs_train, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "seqs_test = tokenizer.texts_to_sequences(reviews_test) # tokenized based on the word_index that was fit on the training words; thus will have oovs\n",
    "padded_test = pad_sequences(seqs_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "## Create a 16-dimensional embedding, for each token (i.e. each tokenized word) in each padded sequence (i.e. each vectorized sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 120, 16)           160000    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1920)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 11526     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 7         \n=================================================================\nTotal params: 171,533\nTrainable params: 171,533\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), \n",
    "    # must specifiy input_length, \n",
    "    # i.e. the length of each sequence that is fed to the model. \n",
    "    # In this case, the length of each sequence is 120 (since we used padding with maxlen=max_length=120)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "    # Just one output node, with activation \"sigmoid\" to push to 0 or 1, corresponding to the two classes (positive or negative)\n",
    "])\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics =[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25000,)"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/10\n25000/25000 [==============================] - 10s 381us/sample - loss: 0.5012 - accuracy: 0.7348 - val_loss: 0.3607 - val_accuracy: 0.8393\nEpoch 2/10\n25000/25000 [==============================] - 8s 313us/sample - loss: 0.2408 - accuracy: 0.9066 - val_loss: 0.3663 - val_accuracy: 0.8420\nEpoch 3/10\n25000/25000 [==============================] - 8s 310us/sample - loss: 0.0913 - accuracy: 0.9778 - val_loss: 0.4456 - val_accuracy: 0.8295\nEpoch 4/10\n25000/25000 [==============================] - 8s 311us/sample - loss: 0.0261 - accuracy: 0.9964 - val_loss: 0.5213 - val_accuracy: 0.8276\nEpoch 5/10\n25000/25000 [==============================] - 8s 317us/sample - loss: 0.0090 - accuracy: 0.9990 - val_loss: 0.5804 - val_accuracy: 0.8277\nEpoch 6/10\n25000/25000 [==============================] - 8s 327us/sample - loss: 0.0031 - accuracy: 0.9998 - val_loss: 0.6430 - val_accuracy: 0.8288\nEpoch 7/10\n25000/25000 [==============================] - 8s 327us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6921 - val_accuracy: 0.8275\nEpoch 8/10\n25000/25000 [==============================] - 8s 326us/sample - loss: 5.1721e-04 - accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.8282\nEpoch 9/10\n25000/25000 [==============================] - 8s 322us/sample - loss: 2.8577e-04 - accuracy: 1.0000 - val_loss: 0.7633 - val_accuracy: 0.8294\nEpoch 10/10\n25000/25000 [==============================] - 8s 321us/sample - loss: 1.7236e-04 - accuracy: 1.0000 - val_loss: 0.7962 - val_accuracy: 0.8293\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x163cdb550>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded_train, labels_train, epochs = num_epochs, validation_data=(padded_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights of the embedding layer are a lookup table, where each row is the 16-dimensional embedding of each of the 10000 words in our word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tensorflow.python.keras.layers.embeddings.Embedding object at 0x1691615d0>\n"
    }
   ],
   "source": [
    "embedder = model.layers[0]\n",
    "print(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array([[ 0.02279906,  0.00532914, -0.04177454, ...,  0.03878019,\n         -0.02602139, -0.00987016],\n        [-0.00317021,  0.02098193,  0.01750253, ...,  0.13743086,\n         -0.09832368,  0.00272052],\n        [ 0.00336432,  0.0446123 , -0.01018012, ...,  0.17973503,\n         -0.14524712, -0.02155509],\n        ...,\n        [-0.06268571, -0.00610817,  0.01784846, ..., -0.0132672 ,\n          0.07257691, -0.1105175 ],\n        [ 0.05336674,  0.10014763, -0.11606579, ...,  0.07198753,\n         -0.03517614, -0.02291496],\n        [-0.01840573,  0.02778517, -0.02546399, ...,  0.13565573,\n          0.17252141, -0.05067727]], dtype=float32)]"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "embedder.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10000, 16)"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "embedder.get_weights()[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(None, 120)"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "embedder.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(None, 120, 16)"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "embedder.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n[   0    0    0   12   14   33  425  392   18   90   28    1    9   32\n 1366 3585   40  486    1  197   24   85  154   19   12  213  329   28\n   66  247  215    9  477   58   66   85  114   98   22 5675   12 1322\n  643  767   12   18    7   33  400 8170  176 2455  416    2   89 1231\n  137   69  146   52    2    1 7577   69  229   66 2933   16    1 2904\n    1    1 1479 4940    3   39 3900  117 1584   17 3585   14  162   19\n    4 1231  917 7917    9    4   18   13   14 4139    5   99  145 1214\n   11  242  683   13   48   24  100   38   12 7181 5515   38 1366    1\n   50  401   11   98 1197  867  141   10]\n? ? ? this was an absolutely terrible movie don't be UNK in by christopher walken or michael UNK both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie's ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the UNK rebels were making their cases for UNK maria UNK UNK appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor's like christopher UNK good name i could barely sit through it\n"
    }
   ],
   "source": [
    "reverse_word_index = dict([(value,key) for key,value in word_index.items()])\n",
    "def decode_review(paddedseq):\n",
    "    return \" \".join([reverse_word_index.get(token, \"?\") for token in paddedseq]) \n",
    "    # Rather than dict[key], dict.get(key) lets us return \"?\" if a certain key is missing.\n",
    "    # THus, all the zeros in the paddedseq (i.e. the pad characters) will be \"?\"\n",
    "print(reviews_train[0])\n",
    "print(padded_train[0])\n",
    "print(decode_review(padded_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "\n",
    "  embeddings = embedder.get_weights()[0][word_num] \n",
    "  # lookup table - visit the row of the table which corresponds to the vector of that word\n",
    "  \n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tsv files in the tensorflow embedding projector to see the word embeddings in 3D embedding space\n",
    "https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}