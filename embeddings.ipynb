{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds \n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n 'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n 'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\",  as_supervised = True, with_info = True) \n",
    "# imdb is a dict containing three different <tf.data.Dataset>,\n",
    "# each of which contain two tensors in the default format of (tensor containing the input, tensor containing the label)\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Large Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\nFeaturesDict({\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n    'text': Text(shape=(), dtype=tf.string),\n})\n{'test': <tfds.core.SplitInfo num_examples=25000>, 'train': <tfds.core.SplitInfo num_examples=25000>, 'unsupervised': <tfds.core.SplitInfo num_examples=50000>}\n"
    }
   ],
   "source": [
    "print(info.description)\n",
    "print(info.features)\n",
    "print(info.splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each of the 25000 records in the <tf.data.Dataset> (for the train set and test set) is stored as a FeaturesDict. \n",
    "* The FeaturesDict consists of a **string tensor** called \"text\" (containing the review) and an **integer tensor** called \"label\"(containing the label). \n",
    "* We need to convert the string tensor and the integer tensor of each record to a np array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "reviews_train = []\n",
    "labels_train = []\n",
    "reviews_test = []\n",
    "labels_test = []\n",
    "\n",
    "for review, label in train_data:\n",
    "\n",
    "    reviews_train.append(review.numpy().decode(\"utf8\"))\n",
    "    # Tensors are explicitly converted to np arrays using their .numpy() method.\n",
    "    # review.numpy() is b\"This was an absolutely terrible movie. Don't ...\" , of <class 'bytes'>\n",
    "\n",
    "    # A string is a sequence of characters, i.e. unicode symbols that can't be directly stored on disk. \n",
    "    # A byte string is a sequence of bytes - things that can be stored on disk. \n",
    "    # The mapping between bytes and unicode symbols is an encoding.\n",
    "    # There are many different types of encodings. \n",
    "    # We need to convert the byte string to an actual string, using the decode() functoin of the byte string.\n",
    "\n",
    "\n",
    "for review, label in test_data:\n",
    "    reviews_test.append(review.numpy().decode(\"utf8\"))\n",
    "    labels_test.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n"
    }
   ],
   "source": [
    "print(reviews_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"UNK\" # \"UNK\" if the word is not part of the vocab_size words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and vectorize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token= oov_tok)\n",
    "tokenizer.fit_on_texts(reviews_train)\n",
    "word_index = tokenizer.word_index\n",
    "seqs_train = tokenizer.texts_to_sequences(reviews_train)\n",
    "padded_train = pad_sequences(seqs_train, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "seqs_test = tokenizer.texts_to_sequences(reviews_test)\n",
    "padded_test = pad_sequences(seqs_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value,key) for key,value in word_index.items()])\n",
    "def decode_review(paddedseq):\n",
    "    return \" \".join([reverse_word_index.get(token, \"?\") for token in paddedseq]) \n",
    "    # Rather than dict[key], dict.get(key) lets us return \"?\" if a certain key is missing.\n",
    "    # THus, all the zeros in the paddedseq (i.e. the pad characters) will be \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "? ? ? this was an absolutely terrible movie don't be UNK in by christopher walken or michael UNK both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie's ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the UNK rebels were making their cases for UNK maria UNK UNK appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor's like christopher UNK good name i could barely sit through it\n"
    }
   ],
   "source": [
    "print(decode_review(padded_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([   0,    0,    0,   12,   14,   33,  425,  392,   18,   90,   28,\n          1,    9,   32, 1366, 3585,   40,  486,    1,  197,   24,   85,\n        154,   19,   12,  213,  329,   28,   66,  247,  215,    9,  477,\n         58,   66,   85,  114,   98,   22, 5675,   12, 1322,  643,  767,\n         12,   18,    7,   33,  400, 8170,  176, 2455,  416,    2,   89,\n       1231,  137,   69,  146,   52,    2,    1, 7577,   69,  229,   66,\n       2933,   16,    1, 2904,    1,    1, 1479, 4940,    3,   39, 3900,\n        117, 1584,   17, 3585,   14,  162,   19,    4, 1231,  917, 7917,\n          9,    4,   18,   13,   14, 4139,    5,   99,  145, 1214,   11,\n        242,  683,   13,   48,   24,  100,   38,   12, 7181, 5515,   38,\n       1366,    1,   50,  401,   11,   98, 1197,  867,  141,   10],\n      dtype=int32)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "padded_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "## Create a 16-dimensional embedding, for each token (i.e. each tokenized word) in each padded sequence (i.e. each vectorized sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}