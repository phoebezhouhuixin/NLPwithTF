{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'dict'>\nis_sarcastic\nheadline\narticle_link\n"
    }
   ],
   "source": [
    "datastore = []\n",
    "with open(\"sarcasm.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        datastore.append(json.loads(line)) # each line is one json object {}\n",
    "print(type(datastore[0]))\n",
    "for key in datastore[0].keys(): \n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(28619, 28619)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "labels = []\n",
    "headlines = []\n",
    "for item in datastore:\n",
    "    labels.append(item[\"is_sarcastic\"])\n",
    "    headlines.append(item[\"headline\"])\n",
    "\n",
    "len(labels), len(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hyperparameter tuning\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"UNK\"\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_train = headlines[:training_size]\n",
    "headlines_test = headlines[training_size:]\n",
    "labels_train = labels[:training_size]\n",
    "labels_test = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token = oov_tok, num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(headlines_train)\n",
    "word_index = tokenizer.word_index\n",
    "seqs_train = tokenizer.texts_to_sequences(headlines_train)\n",
    "seqs_test = tokenizer.texts_to_sequences(headlines_test)\n",
    "seqs_train = pad_sequences(seqs_train, maxlen=max_length, padding= padding_type, truncating= trunc_type)\n",
    "seqs_test = pad_sequences(seqs_test, maxlen=max_length, padding= padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 100, 16)           160000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 16)                0         \n_________________________________________________________________\ndense (Dense)                (None, 24)                408       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 25        \n=================================================================\nTotal params: 160,433\nTrainable params: 160,433\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 20000 samples, validate on 8619 samples\nEpoch 1/10\n20000/20000 - 5s - loss: 0.6500 - accuracy: 0.6220 - val_loss: 0.5138 - val_accuracy: 0.8105\nEpoch 2/10\n20000/20000 - 3s - loss: 0.4045 - accuracy: 0.8379 - val_loss: 0.3688 - val_accuracy: 0.8416\nEpoch 3/10\n20000/20000 - 4s - loss: 0.3046 - accuracy: 0.8769 - val_loss: 0.3537 - val_accuracy: 0.8412\nEpoch 4/10\n20000/20000 - 4s - loss: 0.2547 - accuracy: 0.9022 - val_loss: 0.3291 - val_accuracy: 0.8591\nEpoch 5/10\n20000/20000 - 4s - loss: 0.2188 - accuracy: 0.9176 - val_loss: 0.3353 - val_accuracy: 0.8538\nEpoch 6/10\n20000/20000 - 3s - loss: 0.1918 - accuracy: 0.9293 - val_loss: 0.3416 - val_accuracy: 0.8561\nEpoch 7/10\n20000/20000 - 3s - loss: 0.1717 - accuracy: 0.9365 - val_loss: 0.3556 - val_accuracy: 0.8530\nEpoch 8/10\n20000/20000 - 3s - loss: 0.1521 - accuracy: 0.9443 - val_loss: 0.3719 - val_accuracy: 0.8498\nEpoch 9/10\n20000/20000 - 3s - loss: 0.1363 - accuracy: 0.9521 - val_loss: 0.3941 - val_accuracy: 0.8448\nEpoch 10/10\n20000/20000 - 3s - loss: 0.1261 - accuracy: 0.9546 - val_loss: 0.4179 - val_accuracy: 0.8416\n"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(seqs_train, labels_train, epochs=num_epochs, validation_data=(seqs_test, labels_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}